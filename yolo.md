# Yolo笔记

### 前置知识

#### Yolov1

yolov1奠定了yolo系列算法**“分而治之”**的基调，输入图片被划分为7*7的网格

在inference的过程中，网格只是物体中心点位置的划分之用，并不是对图片进行切片，不会让网格脱离整体的关系。

基本思想是这样：预测框的位置、大小和物体分类都通过CNN暴力predict出来。



#### 卷积神经网络CNN

##### 人工神经网络

###### 神经元

> 神经网络由大量的神经元相互连接而成。每个神经元接受线性组合的输入后，最开始只是简单的线性加权，后来给每个神经元加上了非线性的激活函数，从而进行非线性变换后输出。每两个神经元之间的连接代表加权值，称之为权重（weight）。不同的权重和激活函数，则会导致神经网络不同的输出。

<img src="pics/image-20221012203912621.png" alt="image-20221012203912621" style="zoom:50%;" />

  基本wx + b的形式，其中

- $$x_1,x_2$$表示输入向量
- $$w_1,w_2$$为权重，几个输入则意味着有几个权重，即每个输入都被赋予一个权重
- b为偏置bias
- g(z) 为激活函数
- a 为输出

###### 激活函数

常用的非线性激活函数有sigmoid、tanh、relu等等，前两者sigmoid/tanh比较常见于全连接层，后者relu常见于卷积层。

`sigmoid`:
$$
g(z)=\frac{1}{1+e^{-z}}
$$
z是一个线性组合，比如z可以等于：
$$
z=b+w_1x_1+w_2x_2
$$
<img src="pics/image-20221012204730935.png" alt="image-20221012204730935" style="zoom: 50%;" />

**sigmoid函数的**功能是相当于把一个实数压缩至0到1之间。当z是非常大的正数时，\**g(z)\**会趋近于1，而z是非常小的负数时，则g(z)会趋近于0**。

sigmoid函数可以视为一种分类概率函数，

###### 神经网络

三层神经网络结构

<img src="pics/image-20221012224000878.png" alt="image-20221012224000878" style="zoom:50%;" />

上图中最左边的原始输入信息称之为输入层，最右边的神经元称之为输出层（上图中输出层只有一个神经元），中间的叫隐藏层。

> 输入层（Input layer），众多神经元（Neuron）接受大量非线形输入讯息。输入的讯息称为输入向量。
> 输出层（Output layer），讯息在神经元链接中传输、分析、权衡，形成输出结果。输出的讯息称为输出向量。
> 隐藏层（Hidden layer），简称“隐层”，是输入层和输出层之间众多神经元和链接组成的各个层面。如果有多个隐藏层，则意味着多个激活函数。

##### CNN之卷积计算层

###### CNN怎么进行识别

对于CNN来说，它是一块一块地来进行比对。它拿来比对的这个“小块”我们称之为Features（特征）。在两幅图中大致相同的位置找到一些粗糙的特征进行匹配，CNN能够更好的看到两幅图的相似性，相比起传统的整幅图逐一比对的方法。

###### 什么是卷积

> 对图像（不同的数据窗口数据）和滤波矩阵（一组固定的权重：因为每个神经元的多个权重固定，所以又可以看做一个恒定的滤波器filter）做**内积**（逐个元素相乘再求和）的操作就是所谓的『卷积』操作，也是卷积神经网络的名字来源。

除去中间层和输出层，中间的层可以视为一个滤波器，许多的滤波器叠加就形成了卷积层