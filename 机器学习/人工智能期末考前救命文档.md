## 启发式搜索

### A*算法

### 贪婪优先

### Minimax算法

### alpha-beta 搜索

## 蒙特卡洛树搜索

蒙特卡洛树搜索（Monte Carlo Tree Search，MCTS）是一种用于决策过程的启发式搜索算法，尤其在博弈论和复杂的决策问题中非常有效。它结合了蒙特卡洛方法的随机性和树搜索的结构性，能在状态空间巨大和搜索深度深的情况下，找到较优解。MCTS主要包括四个步骤：选择、扩展、模拟和反向传播。

### 1. 选择（Selection）
从根节点开始，根据某种策略（如UCB1，Upper Confidence Bound 1）选择一个子节点。策略通常是基于节点的访问次数和平均奖励来平衡探索与利用。在MCTS中，一个常用的选择策略是UCB1公式：
$$ \text{UCB1}(v) = \overline{X}_v + C \sqrt{\frac{\ln N_v}{n_v}} $$
其中：

- $ \overline{X}_v $ 是节点v的平均奖励。
- $ N_v $ 是父节点的访问次数。
- $ n_v $ 是节点v的访问次数。
- $ C $ 是一个常数，控制探索和利用的平衡。

### 2. 扩展（Expansion）
当选择过程到达一个叶节点（即尚未完全展开的节点），从该节点选择一个未被访问的子节点，并将其添加到树中。

### 3. 模拟（Simulation）
从新扩展的节点开始，进行模拟（也称为Playout或Rollout）。模拟阶段会随机选择动作，直到达到游戏结束或达到预设的深度。这个过程是为了估计该节点的胜率。

### 4. 反向传播（Backpropagation）
将模拟结果从新扩展的节点反向传播到根节点，更新路径上所有节点的统计数据，如访问次数和平均奖励。

### 重复
以上四个步骤不断重复，直到满足停止条件（如计算时间、模拟次数等）。最后，从根节点选择访问次数最多的子节点作为最佳决策。

### MCTS算法的伪代码

```pseudo
function MCTS(root):
    while computational budget not exhausted:
        leaf = TreePolicy(root)
        simulation_result = DefaultPolicy(leaf)
        Backup(leaf, simulation_result)
    return BestChild(root)

function TreePolicy(node):
    while node is not terminal:
        if node not fully expanded:
            return Expand(node)
        else:
            node = BestChild(node, C)
    return node

function Expand(node):
    choose an untried action a from node
    add a new child node from node with action a
    return new child node

function BestChild(node, C):
    return child with highest UCB1 value

function DefaultPolicy(node):
    while node is not terminal:
        node = random child of node
    return reward of final node

function Backup(node, reward):
    while node is not null:
        update node statistics with reward
        node = parent of node
```

### 应用领域
蒙特卡洛树搜索在很多领域都取得了成功，特别是在以下几方面：
- 博弈论：如围棋、国际象棋等。
- 规划和优化问题：如机器人路径规划。
- 决策过程：如决策支持系统。

MCTS的优势在于它不需要先验知识，适应性强，可以在有限计算资源下提供较好的解。然而，由于它依赖于随机模拟，在某些特定情况下可能会遇到效率瓶颈。

## 无监督学习

### K-means 聚类 	

K-means是一种广泛使用的聚类算法，它的目标是将n个数据点分配到k个聚类中，使得每个数据点归属的聚类中心与其距离之和最小。这里的“聚类中心”是该聚类中所有点的均值。K-means算法简单高效，适用于处理大数据集。然而，K-means算法也有一定的局限性，比如对初始聚类中心的选择敏感，可能陷入局部最优，而且要求预先指定聚类的数量k。

K-means算法的基本步骤如下：

1. **初始化**：随机选择k个数据点作为初始聚类中心。

2. **分配步骤**：对于每个数据点，计算其与各个聚类中心的距离，并将其分配到最近的聚类中心所代表的聚类。

3. **更新步骤**：重新计算每个聚类的中心，即计算该聚类所有点的均值作为新的聚类中心。

4. **迭代**：重复步骤2和步骤3，直到聚类中心不再发生变化，或者变化小于某个阈值，或者达到预设的迭代次数。

K-means算法的几个关键点：

- **距离计算**：K-means通常使用欧氏距离作为距离度量，但也可以使用其他距离度量。

- **聚类数量k的选择**：k的选择对结果有很大影响。实践中，可能需要根据问题的具体背景预先定义，或者使用诸如肘方法（Elbow Method）等技术确定更合适的k值。

- **初始化的影响**：为减少对初始聚类中心选择的依赖，可以多次运行算法，选取最佳结果，或者使用改进的初始化方法，如K-means++。

- **K-means++初始化**：这是一种特别的初始化方式，首先随机选择一个点作为第一个聚类中心，然后每次选择一个新的数据点作为新的聚类中心，选择概率与该点到现有聚类中心的距离成正比，直到选出k个聚类中心。这种方法能够提高算法的收敛速度和聚类质量。

- **收敛性**：尽管K-means算法可以保证在每次迭代中减少数据点与其所属聚类中心的距离，但只能保证找到局部最优解。

K-means算法因其简单和效率在许多领域被广泛应用，包括市场细分、社交网络分析、计算机视觉和图像分析等。

## 有监督学习

### 决策树

决策树是一种常用的机器学习算法，在构建决策树的过程中，需要选择特征进行节点划分，以便最大化分类或回归任务的效果。

#### 信息增益（Information Gain）

信息增益基于熵（Entropy）的减少来选择特征进行划分。

##### 计算方法：
$$
\text{Information Gain}(D, A) = \text{Entropy}(D) - \sum_{v \in \text{Values}(A)} \frac{|D_v|}{|D|} \text{Entropy}(D_v)
$$

其中，$ \text{Entropy}(D) $ 是数据集 D 的熵， $ D_v $ 是特征 A 取值为 v 的数据子集。

##### 优缺点：
- 直观简单，计算方便。
- 在许多情况下效果良好，尤其是二分类问题。

- 偏向于选择取值较多的特征，容易导致过拟合。

##### 例子说明

假设我们有一个简单的数据集，用于判断天气条件是否适合打网球。数据集包含以下特征：天气（Weather）、温度（Temperature）、湿度（Humidity）和风速（Windy），以及对应的标签（是否适合打网球）。

| 天气（Weather）  | 温度（Temperature） | 湿度（Humidity） | 风速（Windy） | 是否适合打网球（Play Tennis） |
| ---------------- | ------------------- | ---------------- | ------------- | ----------------------------- |
| 晴朗（Sunny）    | 热（Hot）           | 高（High）       | 否（No）      | 否（No）                      |
| 晴朗（Sunny）    | 热（Hot）           | 高（High）       | 是（Yes）     | 否（No）                      |
| 阴天（Overcast） | 热（Hot）           | 高（High）       | 否（No）      | 是（Yes）                     |
| 雨天（Rainy）    | 温暖（Mild）        | 高（High）       | 否（No）      | 是（Yes）                     |
| 雨天（Rainy）    | 凉爽（Cool）        | 正常（Normal）   | 否（No）      | 是（Yes）                     |
| 雨天（Rainy）    | 凉爽（Cool）        | 正常（Normal）   | 是（Yes）     | 否（No）                      |
| 阴天（Overcast） | 凉爽（Cool）        | 正常（Normal）   | 是（Yes）     | 是（Yes）                     |
| 晴朗（Sunny）    | 温暖（Mild）        | 高（High）       | 否（No）      | 否（No）                      |
| 晴朗（Sunny）    | 凉爽（Cool）        | 正常（Normal）   | 否（No）      | 是（Yes）                     |
| 雨天（Rainy）    | 温暖（Mild）        | 正常（Normal）   | 否（No）      | 是（Yes）                     |
| 晴朗（Sunny）    | 温暖（Mild）        | 正常（Normal）   | 是（Yes）     | 是（Yes）                     |
| 阴天（Overcast） | 温暖（Mild）        | 高（High）       | 是（Yes）     | 是（Yes）                     |
| 阴天（Overcast） | 热（Hot）           | 正常（Normal）   | 否（No）      | 是（Yes）                     |
| 雨天（Rainy）    | 温暖（Mild）        | 高（High）       | 是（Yes）     | 否（No）                      |

###### 计算信息增益

1. **计算熵（Entropy）**
   熵是数据集的不确定性度量。数据集D的熵计算公式为：
   $$
   \text{Entropy}(D) = - \sum_{i=1}^c p_i \log_2(p_i)
   $$
   其中，$ p_i $ 是第 i 类的概率。

   在这个例子中，我们有两类：“是（Yes）”和“否（No）”。
   - 是（Yes）：9 个
   - 否（No）：5 个

   总的熵为：
   $$
   \text{Entropy}(D) = - \left( \frac{9}{14} \log_2 \left( \frac{9}{14} \right) + \frac{5}{14} \log_2 \left( \frac{5}{14} \right) \right) \approx 0.940
   $$

2. **计算特征“天气（Weather）”的条件熵**
   “天气（Weather）”有三个取值：晴朗（Sunny）、阴天（Overcast）和雨天（Rainy）。

   - **晴朗（Sunny）**：5 个样本，其中 3 个否（No），2 个是（Yes）
     $$
     \text{Entropy(Sunny)} = - \left( \frac{3}{5} \log_2 \left( \frac{3}{5} \right) + \frac{2}{5} \log_2 \left( \frac{2}{5} \right) \right) \approx 0.971
     $$

   - **阴天（Overcast）**：4 个样本，全部是（Yes）
     $$
     \text{Entropy(Overcast)} = - \left( \frac{4}{4} \log_2 \left( \frac{4}{4} \right) \right) = 0
     $$

   -  **雨天（Rainy）**：5 个样本，其中 2 个否（No），3 个是（Yes）
     $$
     \text{Entropy(Rainy)} = - \left( \frac{2}{5} \log_2 \left( \frac{2}{5} \right) + \frac{3}{5} \log_2 \left( \frac{3}{5} \right) \right) \approx 0.971
     $$

   条件熵为：
   $$
   \text{Entropy}(D|Weather) = \frac{5}{14} \times 0.971 + \frac{4}{14} \times 0 + \frac{5}{14} \times 0.971 \approx 0.693
   $$

3. **计算信息增益**
   信息增益为初始熵与条件熵之差：
   $$
   \text{Information Gain}(D, \text{Weather}) = \text{Entropy}(D) - \text{Entropy}(D|\text{Weather}) \\\approx 0.940 - 0.693 = 0.247
   $$

通过类似的方法，可以计算其他特征（如温度、湿度、风速）的信息增益，选择信息增益最大的特征进行划分。



#### 增益比（Gain Ratio）
##### 计算方法：
$$
\text{Gain Ratio}(D, A) = \frac{\text{Information Gain}(D, A)}{\text{Split Information}(A)}
$$

其中，$ \text{Split Information}(A) $ 计算如下：$$ \text{Split Information}(A) = - \sum_{v \in \text{Values}(A)} \frac{|D_v|}{|D|} \log_2 \left( \frac{|D_v|}{|D|} \right) $$

##### 优点：
- 解决了信息增益偏向取值较多特征的问题。

##### 缺点：
- 增加了计算复杂度。
- 可能在某些情况下对数据噪声较为敏感。

##### 例子说明

假设我们有一个简单的数据集，用于判断天气条件是否适合打网球。数据集包含以下特征：天气（Weather）、温度（Temperature）、湿度（Humidity）和风速（Windy），以及对应的标签（是否适合打网球）。

| 天气（Weather）  | 是否适合打网球（Play Tennis） |
| ---------------- | ----------------------------- |
| 晴朗（Sunny）    | 否（No）                      |
| 晴朗（Sunny）    | 否（No）                      |
| 阴天（Overcast） | 是（Yes）                     |
| 雨天（Rainy）    | 是（Yes）                     |
| 雨天（Rainy）    | 是（Yes）                     |
| 雨天（Rainy）    | 否（No）                      |
| 阴天（Overcast） | 是（Yes）                     |
| 晴朗（Sunny）    | 否（No）                      |
| 晴朗（Sunny）    | 是（Yes）                     |
| 雨天（Rainy）    | 是（Yes）                     |
| 晴朗（Sunny）    | 是（Yes）                     |
| 阴天（Overcast） | 是（Yes）                     |
| 阴天（Overcast） | 是（Yes）                     |
| 雨天（Rainy）    | 否（No）                      |

###### 计算信息增益

1. 计算熵（Entropy）
   初始数据集D的熵：
   $$
   \text{Entropy}(D) = - \left( \frac{9}{14} \log_2 \left( \frac{9}{14} \right) + \frac{5}{14} \log_2 \left( \frac{5}{14} \right) \right) \approx 0.940
   $$

2. **计算特征“天气（Weather）”的条件熵**
   “天气（Weather）”有三个取值：晴朗（Sunny）、阴天（Overcast）和雨天（Rainy）。

   - **晴朗（Sunny）**：5 个样本，其中 3 个否（No），2 个是（Yes）
     $$
     \text{Entropy(Sunny)} = - \left( \frac{3}{5} \log_2 \left( \frac{3}{5} \right) + \frac{2}{5} \log_2 \left( \frac{2}{5} \right) \right) \approx 0.971
     $$

   - **阴天（Overcast）**：4 个样本，全部是（Yes）
     $$
     \text{Entropy(Overcast)} = - \left( \frac{4}{4} \log_2 \left( \frac{4}{4} \right) \right) = 0
     $$

   - **雨天（Rainy）**：5 个样本，其中 2 个否（No），3 个是（Yes）
     $$
     \text{Entropy(Rainy)} = - \left( \frac{2}{5} \log_2 \left( \frac{2}{5} \right) + \frac{3}{5} \log_2 \left( \frac{3}{5} \right) \right) \approx 0.971
     $$

   条件熵为：
   $$
   \text{Entropy}(D|Weather) = \frac{5}{14} \times 0.971 + \frac{4}{14} \times 0 + \frac{5}{14} \times 0.971 \approx 0.693
   $$

3. **计算信息增益**
   信息增益为初始熵与条件熵之差：
   $$
   \text{Information Gain}(D, \text{Weather}) = \text{Entropy}(D) - \text{Entropy}(D|\text{Weather}) \approx 0.940 - 0.693 = 0.247
   $$

###### 计算增益比

4. **计算分裂信息（Split Information）**
   分裂信息用于度量特征的固有信息量，其计算公式为：
   $$
   \text{Split Information}(A) = - \sum_{v \in \text{Values}(A)} \frac{|D_v|}{|D|} \log_2 \left( \frac{|D_v|}{|D|} \right)
   $$

   对于特征“天气（Weather）”：
   - **晴朗（Sunny）**：5 个样本
     $$
     \frac{5}{14} \log_2 \left( \frac{5}{14} \right) \approx -0.530
     $$

   - **阴天（Overcast）**：4 个样本
     $$
     \frac{4}{14} \log_2 \left( \frac{4}{14} \right) \approx -0.371
     $$

   - **雨天（Rainy）**：5 个样本
     $$
     \frac{5}{14} \log_2 \left( \frac{5}{14} \right) \approx -0.530
     $$

   分裂信息为：
   $$
   \text{Split Information}(Weather) = - \left( \frac{5}{14} \log_2 \left( \frac{5}{14} \right) + \frac{4}{14} \log_2 \left( \frac{4}{14} \right) + \frac{5}{14} \log_2 \left( \frac{5}{14} \right)  \right) \approx 1.431
   $$

5. **计算增益比**
   增益比为信息增益与分裂信息之比：
   $$
   \text{Gain Ratio}(D, \text{Weather}) = \frac{\text{Information Gain}(D, \text{Weather})}{\text{Split Information}(Weather)} \approx \frac{0.247}{1.431} \approx 0.173
   $$

#### 基尼指数（Gini Index）
基尼指数用于衡量数据集的不纯度，是分类树（如 CART）的常用指标。

##### 计算方法：
$$ \text{Gini}(D) = 1 - \sum_{i=1}^m p_i^2 $$

其中，$ p_i $ 是类别 i 在数据集 D 中的概率。

特征 A 的基尼指数计算如下：

$$ \text{Gini}(D, A) = \sum_{v \in \text{Values}(A)} \frac{|D_v|}{|D|} \text{Gini}(D_v) $$

##### 优点：
- **计算简单**：基尼指数的计算比信息增益和增益比更简单，计算成本较低。
- **减少偏好**：基尼指数在分裂时不偏好于取值较多的特征，更加公平地评估不同特征的分裂效果。
- **稳定性**：在实际应用中，基尼指数在处理分类问题时表现稳定，尤其是在处理大型数据集时，能够有效避免过拟合。

##### 缺点：
- 基尼指数对数据分布不均的情况可能表现较差。

基尼指数（Gini Index）是一种衡量数据集不纯度的指标，广泛用于分类问题中的决策树算法。下面通过一个具体的例子说明基尼指数的计算过程。

##### 例子说明

假设我们有一个简单的数据集，用于判断天气条件是否适合打网球。数据集包含以下特征：天气（Weather）和标签（是否适合打网球）。

| 天气（Weather）  | 是否适合打网球（Play Tennis） |
| ---------------- | ----------------------------- |
| 晴朗（Sunny）    | 否（No）                      |
| 晴朗（Sunny）    | 否（No）                      |
| 阴天（Overcast） | 是（Yes）                     |
| 雨天（Rainy）    | 是（Yes）                     |
| 雨天（Rainy）    | 是（Yes）                     |
| 雨天（Rainy）    | 否（No）                      |
| 阴天（Overcast） | 是（Yes）                     |
| 晴朗（Sunny）    | 否（No）                      |
| 晴朗（Sunny）    | 是（Yes）                     |
| 雨天（Rainy）    | 是（Yes）                     |
| 晴朗（Sunny）    | 是（Yes）                     |
| 阴天（Overcast） | 是（Yes）                     |
| 阴天（Overcast） | 是（Yes）                     |
| 雨天（Rainy）    | 否（No）                      |

###### 计算基尼指数

1. **计算总体基尼指数**

首先，计算整个数据集的基尼指数。我们有14个样本，其中9个是（Yes），5个否（No）。

基尼指数的计算公式为：
$$
\text{Gini}(D) = 1 - \sum_{i=1}^c p_i^2
$$
其中，$ p_i $ 是第 i 类的概率。

对于这个数据集：
$$
p_{\text{Yes}} = \frac{9}{14}, \quad p_{\text{No}} = \frac{5}{14}
$$

所以总体基尼指数为：
$$
\text{Gini}(D) = 1 - \left( \left( \frac{9}{14} \right)^2 + \left( \frac{5}{14} \right)^2 \right) = 1 - \left( \frac{81}{196} + \frac{25}{196} \right) = 1 - \frac{106}{196} \approx 0.459
$$

2. **计算特征“天气（Weather）”的基尼指数**

“天气（Weather）”有三个取值：晴朗（Sunny）、阴天（Overcast）和雨天（Rainy）。

- **晴朗（Sunny）**：5 个样本，其中 3 个否（No），2 个是（Yes）
  $$
  p_{\text{No}} = \frac{3}{5}, \quad p_{\text{Yes}} = \frac{2}{5}
  $$
  $$
  \text{Gini(Sunny)} = 1 - \left( \left( \frac{3}{5} \right)^2 + \left( \frac{2}{5} \right)^2 \right) = 1 - \left( \frac{9}{25} + \frac{4}{25} \right) = 1 - \frac{13}{25} = 0.48
  $$

- **阴天（Overcast）**：4 个样本，全部是（Yes）
  $$
  p_{\text{Yes}} = \frac{4}{4} = 1, \quad p_{\text{No}} = 0
  $$
  $$
  \text{Gini(Overcast)} = 1 - \left( \left( \frac{4}{4} \right)^2 + 0 \right) = 1 - 1 = 0
  $$

- **雨天（Rainy）**：5 个样本，其中 2 个否（No），3 个是（Yes）
  $$
  p_{\text{No}} = \frac{2}{5}, \quad p_{\text{Yes}} = \frac{3}{5}
  $$
  $$
  \text{Gini(Rainy)} = 1 - \left( \left( \frac{2}{5} \right)^2 + \left( \frac{3}{5} \right)^2 \right) = 1 - \left( \frac{4}{25} + \frac{9}{25} \right) = 1 - \frac{13}{25} = 0.48
  $$

3. **计算特征“天气（Weather）”的加权基尼指数**

特征“天气（Weather）”的加权基尼指数为：
$$
\text{Gini}(D|\text{Weather}) = \frac{5}{14} \times \text{Gini(Sunny)} + \frac{4}{14} \times \text{Gini(Overcast)} + \frac{5}{14} \times \text{Gini(Rainy)}
$$

代入基尼值：
$$
\text{Gini}(D|\text{Weather}) = \frac{5}{14} \times 0.48 + \frac{4}{14} \times 0 + \frac{5}{14} \times 0.48 \approx 0.343
$$

###### 选择最佳分裂特征

通过比较不同特征的加权基尼指数，我们可以选择使**基尼指数最小的特征**进行划分。在这个例子中，如果天气是唯一的特征，则天气的加权基尼指数为 0.343，而总体基尼指数为 0.459。

选择天气作为第一个划分特征，因为它降低了整体数据集的不纯度。



## 反向传播神经网络（BP）

反向传播神经网络（Backpropagation Neural Network，简称BP神经网络）是人工神经网络中最常见的一种训练算法。它是一种通过误差反馈来调整网络权重的监督学习方法，广泛应用于分类、回归等任务中。

![在这里插入图片描述](assets/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2thbmNoaWdv,size_16,color_FFFFFF,t_70.png)

### 算法步骤
BP算法的基本步骤如下：

1. **初始化**：随机初始化网络的权重和偏置。
2. **前向传播**：
   - 输入层接收输入数据并将其传递到隐藏层。
   - 隐藏层通过激活函数计算输出，并将结果传递到下一层（输出层）。
   - 输出层计算最终输出。
3. **计算误差**：计算输出层的误差，即网络输出与期望输出之间的差异。
4. **后向传播**：
   - 从输出层开始，计算每个节点的误差项。
   - 通过链式法则，将误差项逐层传递回去，更新各层的权重和偏置。
5. **更新权重和偏置**：使用梯度下降法，根据误差项调整每个连接的权重和每个节点的偏置。
6. **重复**：重复上述步骤，直到误差达到预定的阈值或达到最大迭代次数。

### 计算过程
#### 1. 前向传播
假设网络有 $L$ 层，第 $l$ 层的输入表示为 $\mathbf{a}^{(l-1)}$，权重矩阵为 $\mathbf{W}^{(l)}$，偏置为 $\mathbf{b}^{(l)}$，激活函数为 $f$。

$$
\mathbf{z}^{(l)} = \mathbf{W}^{(l)} \mathbf{a}^{(l-1)} + \mathbf{b}^{(l)} \\ \mathbf{a}^{(l)} = f(\mathbf{z}^{(l)})
$$
最终输出为 $\mathbf{a}^{(L)}$。

#### 2. 计算误差
假设期望输出为 $\mathbf{y}$，网络输出为 $\mathbf{a}^{(L)}$，则误差函数（如均方误差）为：

$$
E = \frac{1}{2} \sum_{k} (\mathbf{y}_k - \mathbf{a}^{(L)}_k)^2
$$

#### 3. 后向传播
从输出层开始，计算误差项 $\delta^{(L)}$：$$ \delta^{(L)} = (\mathbf{a}^{(L)} - \mathbf{y}) \odot f'(\mathbf{z}^{(L)}) $$

对隐藏层，误差项 $\delta^{(l)}$ 为：$$ \delta^{(l)} = (\mathbf{W}^{(l+1)})^T \delta^{(l+1)} \odot f'(\mathbf{z}^{(l)}) $$

#### 4. 更新权重和偏置
使用学习率 $\eta$ 调整权重和偏置：

$$
\mathbf{b}^{(l)} \leftarrow \mathbf{b}^{(l)} - \eta \delta^{(l)} \\
\mathbf{W}^{(l)} \leftarrow \mathbf{W}^{(l)} - \eta \delta^{(l)} (\mathbf{a}^{(l-1)})^T
$$

### 反向传播的计算过程

假设我们有一个简单的三层神经网络：输入层、一个隐藏层和输出层。为了简化计算，我们使用一个只有一个神经元的隐藏层和一个神经元的输出层。

1. **前向传播（Forward Propagation）**
    - 输入层到隐藏层：
      $$ z_1 = w_1 x + b_1 $$
      $$ a_1 = \sigma(z_1) $$
    - 隐藏层到输出层：
      $$ z_2 = w_2 a_1 + b_2 $$
      $$ a_2 = \sigma(z_2) $$
2. **计算误差（Error Calculation）**
    - 误差函数：$$ E = \frac{1}{2}(y - a_2)^2 $$，其中，$ y $ 是期望输出，$ a_2 $ 是实际输出。
3. **后向传播（Backward Propagation）**
    - 输出层误差项：$$ \delta_2 = (a_2 - y) \cdot \sigma'(z_2) $$
    - 隐藏层误差项：$$ \delta_1 = \delta_2 \cdot w_2 \cdot \sigma'(z_1) $$
4. **更新权重和偏置**
    - 学习率为 $ \eta $：
    - $$ w_2 \leftarrow w_2 - \eta \cdot \delta_2 \cdot a_1 $$
    - $$ b_2 \leftarrow b_2 - \eta \cdot \delta_2 $$
    - $$ w_1 \leftarrow w_1 - \eta \cdot \delta_1 \cdot x $$
    - $$ b_1 \leftarrow b_1 - \eta \cdot \delta_1 $$

### 实际例子

假设我们有以下数据：
- 输入 $ x = 0.5 $
- 期望输出 $ y = 0.8 $
- 初始权重和偏置：$ w_1 = 0.4 $, $ b_1 = 0.1 $, $ w_2 = 0.3 $, $ b_2 = -0.2 $
- 学习率 $ \eta = 0.1 $
- 激活函数使用Sigmoid函数 $ \sigma(z) = \frac{1}{1 + e^{-z}} $

#### 1. 前向传播

计算隐藏层输出：
$$ z_1 = w_1 \cdot x + b_1 = 0.4 \cdot 0.5 + 0.1 = 0.3 $$
$$ a_1 = \sigma(z_1) = \frac{1}{1 + e^{-0.3}} \approx 0.5744 $$

计算输出层输出：
$$ z_2 = w_2 \cdot a_1 + b_2 = 0.3 \cdot 0.5744 - 0.2 \approx -0.0277 $$
$$ a_2 = \sigma(z_2) = \frac{1}{1 + e^{0.0277}} \approx 0.4931 $$

#### 2. 计算误差
$$ E = \frac{1}{2}(y - a_2)^2 = \frac{1}{2}(0.8 - 0.4931)^2 \approx 0.0471 $$

#### 3. 后向传播

计算输出层误差项：
$$ \delta_2 = (a_2 - y) \cdot \sigma'(z_2) = (0.4931 - 0.8) \cdot 0.4931 \cdot (1 - 0.4931) \approx -0.0766 $$

计算隐藏层误差项：
$$ \delta_1 = \delta_2 \cdot w_2 \cdot \sigma'(z_1) = -0.0766 \cdot 0.3 \cdot 0.5744 \cdot (1 - 0.5744) \approx -0.0055 $$

#### 4. 更新权重和偏置

$$ w_2 \leftarrow w_2 - \eta \cdot \delta_2 \cdot a_1 = 0.3 - 0.1 \cdot (-0.0766) \cdot 0.5744 \approx 0.3034 $$
$$ b_2 \leftarrow b_2 - \eta \cdot \delta_2 = -0.2 - 0.1 \cdot (-0.0766) \approx -0.1923 $$
$$ w_1 \leftarrow w_1 - \eta \cdot \delta_1 \cdot x = 0.4 - 0.1 \cdot (-0.0055) \cdot 0.5 \approx 0.4003 $$
$$ b_1 \leftarrow b_1 - \eta \cdot \delta_1 = 0.1 - 0.1 \cdot (-0.0055) \approx 0.1006 $$



## 卷积神经网络CNN

这个不会不如死了算了



## 距离计算公式

1. **欧氏距离（Euclidean Distance）**：
   - 这是最常见的距离度量方式，适用于多数几何空间中的距离计算。计算公式为：$$ d(p, q) = \sqrt{\sum_{i=1}^n (p_i - q_i)^2} $$
   - 其中，$ p $ 和 $ q $ 是两个点，$ n $ 是维度数。

2. **曼哈顿距离（Manhattan Distance）**：
   - 又称为城市街区距离（因为它模拟了一个人在规则的街区网络中从一个点移动到另一个点的距离）。该距离是各坐标的绝对值差的总和：$$ d(p, q) = \sum_{i=1}^n |p_i - q_i| $$

3. **切比雪夫距离（Chebyshev Distance）**：
   - 它是无限维空间中的度量，给出的是各个坐标距离的最大值：$$ d(p, q) = \max_i |p_i - q_i| $$

4. **余弦相似度（Cosine Similarity）**：
   - 主要用于度量两个向量的夹角，常用于文本分析中。相似度计算公式为：$$ \cos(\theta) = \frac{p \cdot q}{\|p\| \|q\|} $$
   - 其中，点积 $\cdot$ 表示两个向量的点积，$\|p\|$ 和 $\|q\|$ 是向量的模。余弦距离由1减去余弦相似度得到。

5. **汉明距离（Hamming Distance）**：
   - 用于度量两个等长字符串之间的距离，它等于将一个字符串变换为另一个字符串所需的最少字符替换次数。

6. **杰卡德相似度（Jaccard Similarity）**：
   - 用于比较有限样本集之间的相似性和多样性。计算公式是两集合交集大小与并集大小的比例：$$ J(A, B) = \frac{|A \cap B|}{|A \cup B|} $$
   - 其中，$ A $ 和 $ B $ 是两个集合。

7. **马氏距离（Mahalanobis Distance）**：
   - 它是由印度统计学家马哈拉诺比斯提出的，考虑了数据的协方差。适合用于考虑数据内部结构的情况。计算公式为：$$ d(\mathbf{x}, \mathbf{y}) = \sqrt{(\mathbf{x} - \mathbf{y})^T S^{-1} (\mathbf{x} - \mathbf{y})} $$
   - 其中，$ \mathbf{x} $ 和 $ \mathbf{y} $ 是向量，$ S $ 是数据集的协方差矩阵。
